{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ee632e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56ad88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark==3.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7400ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hadoop distcp \\\n",
    "#         -D fs.s3a.bucket.mlops-backet-16102023.endpoint=storage.yandexcloud.net \\\n",
    "#         -D fs.s3a.bucket.mlops-backet-16102023.acces.key=YCAJEvkQyzISgNsKNE_LOpEQi \\\n",
    "#         -D fs.s3a.bucket.mlops-backet-16102023.secret.key=YCObhqtjSfTFzQnSwBOxlHcrX6Q-V6Lr-WMMnvVc \\\n",
    "#         -update -skipcrccheck -numListstatusThreads 10 \\\n",
    "#         s3a://mlops-backet-16102023/2019-09-21.txt \\\n",
    "#     hdfs://rc1a-dataproc-m-quy5gqnsledmjm7d.mdb.yandexcloud.net/user/root/datasets/set01/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46141a4b",
   "metadata": {},
   "source": [
    "В данном notebook представлено описание найденных ошибок. В конце представлен скрипт для очистки данных.\n",
    "В бакет залил 3 проанализированных дата сета + краткую информацию по датасетам</br>\n",
    "Основные ошибки, некорректные данные:</br>\n",
    "1. значения стобцов пропущены, null, nan</br>\n",
    "2. столбец tx_amount принимает по операциям знаечние \"0\"</br>\n",
    "3. стобец terminal_id > 999, в самом первом датасете 2019-08-22.txt был в диапазоне 0-999. в файле 2019-12-20.txt нашел выбросы > 999. 15290 строк таких в файле 2019-12-20.txt</br>\n",
    "4. повторяющиеся значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8697bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7849dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"mlop-validation-dataset\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .config(\"spark.executor.memory\", \"4g\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6ef65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|tranaction_id|        tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|     46988237|2019-09-21 09:45:59|          1|        178|    83.11|        2627159|          30|       0|                0|\n",
      "|     46988238|2019-09-21 19:33:01|          2|        660|    22.15|        2662381|          30|       0|                0|\n",
      "|     46988239|2019-09-21 18:06:19|          3|        732|    36.83|        2657179|          30|       0|                0|\n",
      "|     46988240|2019-09-21 16:56:01|         10|        663|     19.3|        2652961|          30|       0|                0|\n",
      "|     46988241|2019-09-21 05:34:26|         10|        145|   106.51|        2612066|          30|       0|                0|\n",
      "|     46988242|2019-09-21 12:12:51|         11|        337|    53.97|        2635971|          30|       0|                0|\n",
      "|     46988243|2019-09-21 11:05:32|         11|        973|     29.3|        2631932|          30|       0|                0|\n",
      "|     46988244|2019-09-21 15:13:40|         11|        975|    28.59|        2646820|          30|       0|                0|\n",
      "|     46988245|2019-09-21 16:47:20|         12|        522|    88.02|        2652440|          30|       0|                0|\n",
      "|     46988246|2019-09-21 07:57:03|         12|        522|    77.39|        2620623|          30|       0|                0|\n",
      "|     46988247|2019-09-21 14:47:46|         13|         51|    84.32|        2645266|          30|       0|                0|\n",
      "|     46988248|2019-09-21 07:04:56|         13|        440|     43.3|        2617496|          30|       0|                0|\n",
      "|     46988249|2019-09-21 06:49:40|         13|        819|     72.7|        2616580|          30|       0|                0|\n",
      "|     46988250|2019-09-21 19:53:28|         14|        892|     1.46|        2663608|          30|       0|                0|\n",
      "|     46988251|2019-09-21 19:04:13|         15|        505|    52.54|        2660653|          30|       1|                2|\n",
      "|     46988252|2019-09-21 07:34:48|         15|        145|    34.87|        2619288|          30|       0|                0|\n",
      "|     46988253|2019-09-21 06:53:08|         18|        182|    66.25|        2616788|          30|       0|                0|\n",
      "|     46988254|2019-09-21 08:57:22|         18|        561|   107.31|        2624242|          30|       0|                0|\n",
      "|     46988255|2019-09-21 06:49:38|         21|        273|    83.25|        2616578|          30|       0|                0|\n",
      "|     46988256|2019-09-21 07:55:29|         21|        375|    93.28|        2620529|          30|       0|                0|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# чтение txt файла и преобразование в df\n",
    "file_dir = \"/user/root/datasets/fraudset/2019-09-21.txt\"\n",
    "import re\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"{0}\".format(file_dir))\n",
    "header = rdd.take(1)[0]\n",
    "df = rdd.filter(lambda r: r != header) \\\n",
    "        .map(lambda r: r.split(',')) \\\n",
    "        .toDF(re.sub(r'\\s+', '', header).split('|'))\n",
    "\n",
    "df = df.withColumnRenamed('#tranaction_id', 'tranaction_id')\n",
    "\n",
    "df = df.withColumn('tx_amount', df['tx_amount'].cast(FloatType()))\n",
    "\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9fe4dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|tranaction_id|        tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|            0|2019-08-22 06:51:03|          0|        711|    70.91|          24663|           0|       0|                0|\n",
      "|            1|2019-08-22 05:10:37|          0|          0|    90.55|          18637|           0|       0|                0|\n",
      "|            2|2019-08-22 19:05:33|          0|        753|    35.38|          68733|           0|       0|                0|\n",
      "|            3|2019-08-22 07:21:33|          0|          0|    80.41|          26493|           0|       0|                0|\n",
      "|            4|2019-08-22 09:06:17|          1|        981|   102.83|          32777|           0|       0|                0|\n",
      "|            5|2019-08-22 18:41:25|          3|        205|     34.2|          67285|           0|       0|                0|\n",
      "|            6|2019-08-22 03:12:21|          3|          0|     47.2|          11541|           0|       0|                0|\n",
      "|            7|2019-08-22 22:36:40|          6|        809|   139.39|          81400|           0|       0|                0|\n",
      "|            8|2019-08-22 17:23:29|          7|        184|    87.24|          62609|           0|       0|                0|\n",
      "|            9|2019-08-22 21:09:37|          8|        931|     61.7|          76177|           0|       0|                0|\n",
      "|           10|2019-08-22 11:32:42|         10|        663|    40.71|          41562|           0|       1|                2|\n",
      "|           11|2019-08-22 03:09:26|         10|        770|    63.91|          11366|           0|       0|                0|\n",
      "|           12|2019-08-22 15:47:54|         10|          0|    58.89|          56874|           0|       0|                0|\n",
      "|           13|2019-08-22 21:59:20|         10|        649|    89.24|          79160|           0|       0|                0|\n",
      "|           14|2019-08-22 20:55:13|         11|        380|     9.89|          75313|           0|       0|                0|\n",
      "|           15|2019-08-22 16:39:03|         11|        337|    83.36|          59943|           0|       0|                0|\n",
      "|           16|2019-08-22 23:15:07|         11|        973|    35.12|          83707|           0|       0|                0|\n",
      "|           17|2019-08-22 07:39:45|         12|          9|     74.0|          27585|           0|       0|                0|\n",
      "|           18|2019-08-22 05:35:39|         12|        745|   108.63|          20139|           0|       0|                0|\n",
      "|           19|2019-08-22 10:29:16|         12|          9|    84.45|          37756|           0|       0|                0|\n",
      "+-------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# чтение txt файла и преобразование в df\n",
    "import re\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"/user/root/datasets/fraudset/2019-08-22.txt\")\n",
    "header = rdd.take(1)[0]\n",
    "df = rdd.filter(lambda r: r != header) \\\n",
    "        .map(lambda r: r.split(',')) \\\n",
    "        .toDF(re.sub(r'\\s+', '', header).split('|'))\n",
    "\n",
    "df = df.withColumnRenamed('#tranaction_id', 'tranaction_id')\n",
    "\n",
    "df = df.withColumn('tx_amount', df['tx_amount'].cast(FloatType()))\n",
    "\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5513258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tranaction_id: string (nullable = true)\n",
      " |-- tx_datetime: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- terminal_id: string (nullable = true)\n",
      " |-- tx_amount: float (nullable = true)\n",
      " |-- tx_time_seconds: string (nullable = true)\n",
      " |-- tx_time_days: string (nullable = true)\n",
      " |-- tx_fraud: string (nullable = true)\n",
      " |-- tx_fraud_scenario: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() #схема данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c03136",
   "metadata": {},
   "source": [
    "Общий анализ DF, информация по кол-ву строк, мошенническим операциям,легитимным операциям, количеству уникальных и повторяющихся строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f271366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего строк:  46988418\n",
      "Количество повторяющихся строк:  181\n",
      "Количество мошеннических операций:  2527005\n",
      "Количество легитимных операций:  44461413\n",
      "CPU times: user 21.2 ms, sys: 26.4 ms, total: 47.7 ms\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "len_df = df.count() #количество всех строк\n",
    "len_fraud_df = df.filter(\"tx_fraud = 1\").count()\n",
    "len_without_fraud = len_df - len_fraud_df\n",
    "len_distinct_rows = df.distinct().count()\n",
    "print('Всего строк: ', len_df)\n",
    "print('Количество повторяющихся строк: ', len_df - len_distinct_rows)\n",
    "print('Количество мошеннических операций: ', len_fraud_df)\n",
    "print('Количество легитимных операций: ', len_without_fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcbe901",
   "metadata": {},
   "source": [
    "Строки равные нулю, None, Null по столбцам tx_datetime, customer_id, customer_id.</br>\n",
    "По столбцу tx_amount равны 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a329ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 ms, sys: 15.6 ms, total: 37.1 ms\n",
      "Wall time: 3min 52s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tx_datetime</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>terminal_id</th>\n",
       "      <th>tx_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>2041671</td>\n",
       "      <td>884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tx_datetime  customer_id  terminal_id  tx_amount\n",
       "0            0           66      2041671        884"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import col, asc, desc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "string_columns = ['tx_datetime', 'customer_id', 'terminal_id']\n",
    "numeric_columns = ['tx_amount']\n",
    "missing_values = {}\n",
    "for index, column in enumerate(df.columns):\n",
    "    if column in string_columns:\n",
    "        missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull() | (col(column) == 0)).count()\n",
    "        missing_values.update({column:missing_count})\n",
    "    if column in numeric_columns:\n",
    "        missing_count = df.filter((col(column) == 0)).count()\n",
    "        missing_values.update({column:missing_count})\n",
    "        \n",
    "missing_df = pd.DataFrame.from_dict([missing_values])\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1876e3b5",
   "metadata": {},
   "source": [
    "Количество строк со значением Null, пропусками в датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96d6949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пропущенных строк:  0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, trim\n",
    "\n",
    "def to_null(c):\n",
    "    return when(~(col(c).isNull() | isnan(col(c)) | (trim(col(c)) == \"\")), col(c))\n",
    "\n",
    "\n",
    "count_null_rows = df.select([to_null(c).alias(c) for c in df.columns]).na.drop().count()\n",
    "print('Количество строк с пропущенными данными: ', len_df - count_null_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fec083",
   "metadata": {},
   "source": [
    "Очистка датасета, заметил в конце ошибку, я не исключил terminal_id = 0. В скрипте исправления внес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9ac3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строк удалено : 16856\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, trim\n",
    "\n",
    "df_validation = (\n",
    "    df\n",
    "    .filter(col(\"terminal_id\") <= 999)\n",
    "    .filter(col(\"tx_amount\") > 0)\n",
    "    .filter(col(\"customer_id\") > 0)\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "\n",
    "# пропуски и значения Null\n",
    "def to_null_bool(c, dt):\n",
    "    if df == \"float\":\n",
    "        return c.isNull() | isnan(c)\n",
    "    elif df == \"string\":\n",
    "        return ~c.isNull() & (trim(c) != \"\")\n",
    "    else:\n",
    "        return ~c.isNull()\n",
    "\n",
    "# Only keep columns with not empty strings\n",
    "def to_null(c, dt):\n",
    "    c = col(c)\n",
    "    return when(to_null_bool(c, dt), c)\n",
    "\n",
    "\n",
    "df_validation = df_validation.select([to_null(c, dt[1]).alias(c) for c, dt in zip(df_validation.columns, df_validation.dtypes)]).na.drop(how=\"any\")\n",
    "\n",
    "len_val_df = df_validation.count()\n",
    "\n",
    "print('Строк удалено :', len_df - len_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d648f",
   "metadata": {},
   "source": [
    "Запись датасета в формат паркета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"/user/root/datasets/set02/2019-08-22.txt\"\n",
    "# file_name = file_dir.split(\"/\")[len(file_name) - 1]\n",
    "file_name = file_dir.split(\"/\")[len(file_dir.split(\"/\")) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc3cd76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.write.parquet(\"/user/root/datasets/fraudset_parq/df_validation_{0}.parquet\".format(file_name[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9a1e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f0ffa",
   "metadata": {},
   "source": [
    "Итоговый скрипт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9c4ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создать файл month_stat.py\n",
    "#запустить приложение /usr/bin/spark-submit month_stat.py\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, asc, desc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, isnan, when, trim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_info(info, name_file):\n",
    "    with open(r\"{0}\".format(name_file), \"w\") as file:\n",
    "        for k, v in info.items():\n",
    "            file.write('{0} {1} \\n'.format(k, v))\n",
    "            \n",
    "            \n",
    "def to_null(c):\n",
    "    return when(~(col(c).isNull() | isnan(col(c)) | (trim(col(c)) == \"\")), col(c))\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = (\n",
    "        SparkSession\n",
    "            .builder\n",
    "            .appName(\"mlop-validation-dataset\")\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "            .config(\"spark.executor.memory\", \"4g\")\n",
    "            .config(\"spark.driver.memory\", \"4g\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    file_dir = \"/user/root/datasets/fraudset/2022-06-07.txt\" #задал явно, потому нужно будет через *.txt?\n",
    "    file_name = file_dir.split(\"/\")[len(file_dir.split(\"/\")) - 1]\n",
    "\n",
    "    rdd = spark.sparkContext.textFile(\"{0}\".format(file_dir))\n",
    "    header = rdd.take(1)[0]\n",
    "    df = rdd.filter(lambda r: r != header) \\\n",
    "            .map(lambda r: r.split(',')) \\\n",
    "            .toDF(re.sub(r'\\s+', '', header).split('|'))\n",
    "\n",
    "    df = df.withColumnRenamed('#tranaction_id', 'tranaction_id')\n",
    "\n",
    "    df = df.withColumn('tx_amount', df['tx_amount'].cast(FloatType()))\n",
    "    \n",
    "    \n",
    "    len_df = df.count() #количество всех строк\n",
    "    len_fraud_df = df.filter(\"tx_fraud = 1\").count()\n",
    "    len_without_fraud = len_df - len_fraud_df\n",
    "    len_distinct_rows = df.distinct().count()\n",
    "    \n",
    "    df_info = {'count_all_rows: ': len_df,\n",
    "               'count_duplicates_rows: ': len_df - len_distinct_rows,\n",
    "               'count_fraud_rows: ': len_fraud_df,\n",
    "               'count_without_fraud_rows: ': len_without_fraud}\n",
    "                \n",
    "    string_columns = ['tx_datetime', 'customer_id', 'terminal_id']\n",
    "    numeric_columns = ['tx_amount']\n",
    "        \n",
    "    for index, column in enumerate(df.columns):\n",
    "        if column in string_columns:\n",
    "            missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull() | (col(column) == 0)).count()\n",
    "            df_info.update({\"missings_{0}\".format(column):missing_count})\n",
    "        if column in numeric_columns:\n",
    "            missing_count = df.filter((col(column) == 0)).count()\n",
    "            df_info.update({\"missings_{0}\".format(column):missing_count})\n",
    "            \n",
    "    negative_values = df.filter('tx_amount < 0').count()\n",
    "    df_info.update({\"negative_values_tx_amount\":negative_values})\n",
    "    \n",
    "    \n",
    "    df_validation = (\n",
    "        df\n",
    "            .filter(col(\"terminal_id\").between('1', '999'))\n",
    "            .filter(col(\"tx_amount\") > 0)\n",
    "            .filter(col(\"customer_id\") > 0)\n",
    "            .distinct()\n",
    "    )\n",
    "    \n",
    "    df_validation = df_validation.select([to_null(c).alias(c) for c in df.columns]).na.drop()\n",
    "    len_val_df = df_validation.count()\n",
    "    \n",
    "    df_info.update({\"count_del_rows\":len_df - len_val_df})\n",
    "    \n",
    "    write_info(df_info, \"{0}_info.txt\".format(file_name))\n",
    "    \n",
    "    df_validation.write.parquet(\"/user/root/datasets/fraudset_parq/df_validation_{0}.parquet\".format(file_name[:-4]))\n",
    "    \n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b1a12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 ubuntu hadoop 2854479008 2023-11-05 09:09 /user/root/datasets/set01\r\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -ls /user/root/datasets/set01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b4340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#пробую удалить из DF терминалы с 0% мошенничества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a80e577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.82 ms, sys: 0 ns, total: 3.82 ms\n",
      "Wall time: 43.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_only_fraud_terminal_id = (\n",
    "    df_validation\n",
    "    .select(\"terminal_id\", \"tx_fraud\")\n",
    "    .filter(\"tx_fraud = 1\")\n",
    "    .groupBy(\"terminal_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\",\"tx_fraud_1_terminal\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c36f9fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 674 µs, sys: 2.59 ms, total: 3.26 ms\n",
      "Wall time: 27.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_all_transaction_terminal_id = (\n",
    "    df_validation\n",
    "    .select(\"terminal_id\")\n",
    "    .groupBy(\"terminal_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\",\"terminal_cnt_tranaction_id_for\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cffb21b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------+-------------------+\n",
      "|terminal_id|terminal_cnt_tranaction_id_for|tx_fraud_1_terminal|\n",
      "+-----------+------------------------------+-------------------+\n",
      "+-----------+------------------------------+-------------------+\n",
      "\n",
      "CPU times: user 56.6 ms, sys: 26.3 ms, total: 83 ms\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(\n",
    "    df_all_transaction_terminal_id\n",
    "    .join(df_only_fraud_terminal_id, on='terminal_id', how=\"left\")\n",
    "    .filter(\"tx_fraud_1_terminal = 0\")\n",
    "    .limit(10)\n",
    "    .show()\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
