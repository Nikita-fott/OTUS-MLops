{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ce9a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52816957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1999684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.3 pyspark-shell'\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea87203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupId = org.apache.spark\n",
    "# artifactId = spark-sql-kafka-0-10_2.12\n",
    "# version = 3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15ebaa34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://rc1a-dataproc-m-dt27mwn7ayzjtxu7.mdb.yandexcloud.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming-kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1ca8f86b80>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"Streaming-kafka\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.executor.instances\", \"3\")\n",
    "        .config(\"spark.default.parallelism\", \"24\")\n",
    "        .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"true\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1b9fe915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.0.3\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 11.0.21\n",
      "Branch \n",
      "Compiled by user  on 2023-11-07T12:42:26Z\n",
      "Revision \n",
      "Url \n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-shell --version\n",
    "\n",
    "#version kafka 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cb144534",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "684eb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"rc1a-sq626k239t359ent.mdb.yandexcloud.net:9091\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\") \\\n",
    "    .option(\"kafka.sasl.username\", \"mlops\") \\\n",
    "    .option(\"kafka.sasl.password\", \"otus-mlops\") \\\n",
    "    .option(\"subscribe\", \"transactions_sample\") \\\n",
    "    .option(\"kafka.group.id\", \"mlops\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "af9c63c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e71deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "json_schema = StructType([StructField(\"ts\", StringType(), True), \\\n",
    "                          StructField(\"user_id\", StringType(), True), \\\n",
    "                          StructField(\"page_id\", StringType(), True)])\n",
    "\n",
    "\n",
    "# json_schema = StructType([StructField('ts', StringType(), True), \\\n",
    "# StructField('user_id', StructType([StructField('page_id', ArrayType(StructType([ \\\n",
    "# StructField('deviceId', StringType(), True), \\\n",
    "# StructField('measure', StringType(), True), \\\n",
    "# StructField('status', StringType(), True), \\\n",
    "# StructField('temperature', LongType(), True)]), True), True)]), True), \\\n",
    "# StructField('eventId', StringType(), True), \\\n",
    "# StructField('eventOffset', LongType(), True), \\\n",
    "# StructField('eventPublisher', StringType(), True), \\\n",
    "# StructField('eventTime', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "51397fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = streaming_df.selectExpr(\"cast(value as string) as value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39f1d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType,StringType\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], json_schema)).select(\"value.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6cbce9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ts: string, user_id: string, page_id: string]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "45c147b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, split\n",
    "\n",
    "exploded_df = json_expanded_df \\\n",
    "    .select(\"ts\", \"user_id\", \"page_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43899cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useDeprecated = spark.sql.internal.SQLConf.useDeprecatedKafkaOffsetFetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9c9f0d30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query = exploded_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ea4d3212",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "'boolean org.apache.spark.sql.internal.SQLConf.useDeprecatedKafkaOffsetFetching()'\n=== Streaming Query ===\nIdentifier: [id = 90693c2b-c7e6-44b1-8ae9-345d5326e1a0, runId = d866f83b-b745-4158-85ea-2eeb24890eb3]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {}\n\nCurrent State: INITIALIZING\nThread State: RUNNABLE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-885fef5a9f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: 'boolean org.apache.spark.sql.internal.SQLConf.useDeprecatedKafkaOffsetFetching()'\n=== Streaming Query ===\nIdentifier: [id = 90693c2b-c7e6-44b1-8ae9-345d5326e1a0, runId = d866f83b-b745-4158-85ea-2eeb24890eb3]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {}\n\nCurrent State: INITIALIZING\nThread State: RUNNABLE"
     ]
    }
   ],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-shell --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.streaming import StreamingContext\n",
    "# from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# # Create a Spark session\n",
    "# spark = SparkSession.builder.appName(\"KafkaSparkStreaming\").getOrCreate()\n",
    "\n",
    "# # Create a Streaming Context with a batch interval of 5 seconds\n",
    "# ssc = StreamingContext(spark.sparkContext, 5)\n",
    "\n",
    "# # Define Kafka parameters\n",
    "# kafka_params = {\n",
    "#     \"bootstrap.servers\": \"localhost:9092\",\n",
    "#     \"group.id\": \"spark-streaming-group\",\n",
    "#     \"auto.offset.reset\": \"latest\"\n",
    "# }\n",
    "# kafka_topic = \"your_kafka_topic_name\"\n",
    "\n",
    "# # Create a DStream that reads from the Kafka topic\n",
    "# kafka_stream = KafkaUtils.createDirectStream(ssc, [kafka_topic], kafka_params)\n",
    "\n",
    "# # Process the streaming data from Kafka\n",
    "# lines = kafka_stream.map(lambda x: x[1])  # Extract message value\n",
    "# lines.pprint()  # Print the message to the console\n",
    "\n",
    "# # Start the streaming context\n",
    "# ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# from pyspark.streaming import StreamingContext\n",
    "\n",
    "# # Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "# sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "# ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = ssc.socketTextStream(\"rc1a-sq626k239t359ent.mdb.yandexcloud.net\", 9091)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c55e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c95918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import logging\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# import pyspark\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import SQLContext, SparkSession\n",
    "# import pyspark.sql.functions as f\n",
    "# from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "# from pyspark.sql.types import MapType,StringType\n",
    "# from pyspark.sql.functions import from_json\n",
    "# from pyspark.sql.functions import explode, col, split\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     spark = (\n",
    "#         SparkSession\n",
    "#         .builder\n",
    "#         .appName(\"kafka-stream\")\n",
    "#         .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "#         .config(\"spark.executor.cores\", \"2\")\n",
    "#         .config(\"spark.executor.memory\", \"2g\")\n",
    "#         .config(\"spark.executor.instances\", \"3\")\n",
    "#         .config(\"spark.default.parallelism\", \"24\").getOrCreate()\n",
    "#     )\n",
    "\n",
    "#     streaming_df = spark.readStream\\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"rc1a-sq626k239t359ent.mdb.yandexcloud.net:9091\") \\\n",
    "#     .option(\"subscribe\", \"transactions_sample\") \\\n",
    "#     .option(\"startingOffsets\", \"earliest\") \\\n",
    "#     .load()\n",
    "\n",
    "\n",
    "#     json_schema = StructType([StructField(\"ts\", StringType(), True), \\\n",
    "#                           StructField(\"user_id\", StringType(), True), \\\n",
    "#                           StructField(\"page_id\", StringType(), True)])\n",
    "\n",
    "#     json_df = streaming_df.selectExpr(\"cast(value as string) as value\")\n",
    "\n",
    "#     json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], json_schema)).select(\"value.*\")\n",
    "#     exploded_df = json_expanded_df.select(\"ts\", \"user_id\", \"page_id\")\n",
    "\n",
    "#     result = exploded_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"checkpointLocation\",\"checkpoint_dir\") \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .start()\n",
    "\n",
    "#     result.awaitTermination()\n",
    "\n",
    "#     spark.stop()\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
